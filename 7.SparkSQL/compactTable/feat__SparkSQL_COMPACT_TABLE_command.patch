Index: sql/core/src/main/scala/org/apache/spark/sql/execution/command/CompactTableCommand.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CompactTableCommand.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CompactTableCommand.scala
new file mode 100644
--- /dev/null	(date 1632644146916)
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CompactTableCommand.scala	(date 1632644146916)
@@ -0,0 +1,73 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+package org.apache.spark.sql.execution.command
+
+import org.apache.spark.sql.{Row, SaveMode, SparkSession}
+import org.apache.spark.sql.catalyst.TableIdentifier
+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference}
+import org.apache.spark.sql.types.StringType
+
+/**
+ * Compact Table
+ *
+ * @param tableId table Id
+ */
+case class CompactTableCommand(tableId: TableIdentifier, fileNumber: Option[String]) extends
+  LeafRunnableCommand {
+  override val output: Seq[Attribute] =
+    Seq(AttributeReference("compact", StringType)())
+
+  override def run(sparkSession: SparkSession): Seq[Row] = {
+    // get Table by tableIdentifier
+    val table = sparkSession.table(tableId)
+
+    // recalculate partition number
+    var tablePartitionsNumber = fileNumber match {
+      case None => (sparkSession.sessionState
+        .executePlan(table.logicalPlan)
+        .optimizedPlan.stats.sizeInBytes >> 7).toInt
+      case Some(value) => value.toInt
+    }
+    // ( Number of partitions (0) must be positive)
+    tablePartitionsNumber = Math.max(tablePartitionsNumber, 1)
+
+    val tmpTableName = tableId.table + "_tmp"
+
+    // rewrite the files base on the recalculated partition number
+    // FYI:
+    // Here we introduce temp table to solve
+    // "Cannot overwrite table that is also being read from" issue
+
+    table.repartition(tablePartitionsNumber)
+      .write
+      .mode(SaveMode.Overwrite)
+      .saveAsTable(tmpTableName)
+
+    val tmpTable = sparkSession.table(tmpTableName)
+
+    tmpTable
+      .write
+      .mode(SaveMode.Overwrite)
+      .saveAsTable(tableId.table)
+
+    sparkSession.sql(s"DROP TABLE $tmpTableName ;")
+
+    Seq(Row(s"Compact table ${tableId.table} finished"))
+  }
+}
Index: sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4 b/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4
--- a/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4	(revision 540e45c3cc7c64e37aa5c1673c03a0f2d7462878)
+++ b/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4	(date 1632639964174)
@@ -244,6 +244,8 @@
     | SET .*?                                                          #setConfiguration
     | RESET configKey                                                  #resetQuotedConfiguration
     | RESET .*?                                                        #resetConfiguration
+    | COMPACT TABLE target=tableIdentifier partitionSpec?
+    (INTO fileNum=INTEGER_VALUE identifier)?                           #compactTable
     | unsupportedHiveNativeCommands .*?                                #failNativeCommand
     ;
 
@@ -1103,6 +1105,7 @@
     | EXTRACT
     | FIELDS
     | FILEFORMAT
+    | FILES
     | FIRST
     | FOLLOWING
     | FORMAT
@@ -1347,6 +1350,7 @@
     | FILTER
     | FIELDS
     | FILEFORMAT
+    | FILES
     | FIRST
     | FOLLOWING
     | FOR
@@ -1602,6 +1606,7 @@
 FIELDS: 'FIELDS';
 FILTER: 'FILTER';
 FILEFORMAT: 'FILEFORMAT';
+FILES: 'FILES';
 FIRST: 'FIRST';
 FOLLOWING: 'FOLLOWING';
 FOR: 'FOR';
Index: sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala	(revision 540e45c3cc7c64e37aa5c1673c03a0f2d7462878)
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala	(date 1632643595054)
@@ -633,4 +633,19 @@
 
     (ctx.LOCAL != null, finalStorage, Some(DDLUtils.HIVE_PROVIDER))
   }
+
+  /**
+   *
+   * @param ctx the parse tree
+   *
+   */
+  override def visitCompactTable(ctx: CompactTableContext): LogicalPlan = withOrigin(ctx) {
+    val tableName = visitTableIdentifier(ctx.tableIdentifier())
+    val fileNum = if (ctx.INTEGER_VALUE() != null) {
+      Some(ctx.INTEGER_VALUE().getText)
+    } else {
+      None
+    }
+    CompactTableCommand(tableName, fileNum)
+  }
 }
